---
title: "R Notebook"
output: html_notebook
fig_width: 10
fig_height: 10
---

```{r}
knitr::opts_chunk$set(fig.width=22, fig.height=20) 
library(tidyverse)
library(caret)
library(Hmisc)
library(psych)
library(rbin)
```

## Import
Importing Cleaned Data
```{r}
ames = read_csv('./data/train_clean.csv')
#amest = read_csv('./data/test_clean.csv')
ames = ames[,order(colnames(ames))]
#test = test[,order(colnames(test))]

ames = ames %>% dplyr::select(-X1)
#test = test %>% dplyr::select(-Id,-X1)
ames = ames[,order(colnames(ames))] %>% 
  rename('FirstFlrSF' = "1stFlrSF", 'SecFlrSF' = '2ndFlrSF', 'ThreeSeaPorch' = '3SsnPorch')


# test = test[,order(colnames(test))] %>% 
#   rename('FirstFlrSF' = "1stFlrSF", 'SecFlrSF' = '2ndFlrSF', 'ThreeSeaPorch' = '3SsnPorch')


```


## Split into train and validation
```{r}
train.idx = sample(1:nrow(ames), 10*nrow(ames)/10)
ames_train = ames#[train.idx,]
#ames_test = ames[-train.idx,]

```


```{r}
mean(ames_train$SalePrice)
```

### Numerical Features
```{r}
library(ggfortify)
```

```{r}
require(gridExtra)
plot_numerical_bands = function(df, x_name, y_name) {
  x = df[,x_name][[1]]
  y = df[,y_name][[1]]
  model = lm(y ~ x, data=df)
  predicted = predict(model)
  
  
  
  g = ggplot(df, aes(x,y)) +
    geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
    geom_segment(aes(xend = x, yend=predicted), alpha=0.2) +
    geom_point(aes(color=abs(model$residuals))) +
    scale_color_continuous(low = "black", high = "red") +
    guides(color=FALSE) +
    geom_point(aes(y=predicted),shape=1) +
    labs(x = x_name, y = y_name) +
    theme_bw()
  
  # Construction of Confidence Intervals
  
  clower = predict(model, interval='confidence', level=0.95)[,2]
  cupper = predict(model, interval='confidence', level=0.95)[,3]
  
  
  plower = predict(model, interval='prediction', level=0.95)[,2]
  pupper = predict(model, interval='prediction', level=0.95)[,3]
  
  g2 = g + geom_ribbon(aes(ymin = clower, ymax = cupper), alpha=0.3, col = 'red') +
    geom_ribbon(aes(ymin=plower, ymax = pupper), alpha=0.2, col='blue')

  model_name = paste0(y_name,'~',x_name)
  dir.create(paste0("./Linear Model/",model_name))
  
  # Save Scatter Plot
  ggsave(paste0("./Linear Model/",model_name,"/scatter_plot.pdf"), g2)
  
  # Save Diagnostic PLots
  pdf(paste0("./Linear Model/",model_name,"/residual_plots.pdf"))
  par(mfrow=c(2,2))
  plot(model,which=c(1,2,4,6))
  dev.off()
  
  # Save Influence Plot
  pdf(paste0("./Linear Model/",model_name,"/influence.pdf"))
  inf = influencePlot(model)

  dev.off()
  
  
  # plot(x, 
  #      abs(model$residuals), 
  #      xlab = x_name, 
  #      ylab = 'Residual Values Squared', 
  #      main = 'Squared Residuals compared to MSE')
  # abline(h=sqrt(mean(model$residuals^2)), lty = 2, lwd=3, col = 'red')
  # ss = smooth.spline(x, model$residuals^2, cv = TRUE)
  # lines(ss, lwd=2, col='blue')
  # abline(h=mean(ss$y), lwd=2, col='black')
  # legend("topleft", 
  #      c("MSE", "Spline", "Spline Mean"),
  #      lty = c(2, 1, 1), 
  #      col = c("red", "blue", "black"))
}
```



```{r}


ames_train %>% 
  filter(GrLivArea <= 4000) %>% 
  #mutate(SalePrice = log1p(SalePrice), GrLivArea = log1p(GrLivArea)) %>% 
  plot_w_bands(., "TotalSF", "SalePrice")

```

```{r}
require(gridExtra)
plot_cat_bands = function(df, x_name, y_name) {
  x = df[,x_name][[1]]
  y = df[,y_name][[1]]
  model = lm(y ~ x, data=df)
  predicted = predict(model)
  
  
  
  g = ggplot(df, aes(x,y)) +
    geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
    geom_segment(aes(xend = x, yend=predicted), alpha=0.2) +
    geom_point(aes(color=abs(model$residuals))) +
    scale_color_continuous(low = "black", high = "red") +
    guides(color=FALSE) +
    geom_point(aes(y=predicted),shape=1) +
    labs(x = x_name, y = y_name) +
    theme_bw()
  
  # Construction of Confidence Intervals
  
  clower = predict(model, interval='confidence', level=0.95)[,2]
  cupper = predict(model, interval='confidence', level=0.95)[,3]
  
  
  plower = predict(model, interval='prediction', level=0.95)[,2]
  pupper = predict(model, interval='prediction', level=0.95)[,3]
  
  g2 = g + geom_ribbon(aes(ymin = clower, ymax = cupper), alpha=0.3, col = 'red') +
    geom_ribbon(aes(ymin=plower, ymax = pupper), alpha=0.2, col='blue')

  model_name = paste0(y_name,'~',x_name)
  dir.create(paste0("./Linear Model/",model_name))
  
  # Save Scatter Plot
  ggsave(paste0("./Linear Model/",model_name,"/scatter_plot.pdf"), g2)
  
  # Save Diagnostic PLots
  pdf(paste0("./Linear Model/",model_name,"/residual_plots.pdf"))
  par(mfrow=c(2,2))
  plot(model,which=c(1,2,4,6))
  dev.off()
  
  # Save Influence Plot
  pdf(paste0("./Linear Model/",model_name,"/influence.pdf"))
  inf = influencePlot(model)

  dev.off()
  
  
  # plot(x, 
  #      abs(model$residuals), 
  #      xlab = x_name, 
  #      ylab = 'Residual Values Squared', 
  #      main = 'Squared Residuals compared to MSE')
  # abline(h=sqrt(mean(model$residuals^2)), lty = 2, lwd=3, col = 'red')
  # ss = smooth.spline(x, model$residuals^2, cv = TRUE)
  # lines(ss, lwd=2, col='blue')
  # abline(h=mean(ss$y), lwd=2, col='black')
  # legend("topleft", 
  #      c("MSE", "Spline", "Spline Mean"),
  #      lty = c(2, 1, 1), 
  #      col = c("red", "blue", "black"))
}
```

```{r}

sf_bins = rbin_manual(ames_train, SalePrice, TotalSF, c(1e2,1e3,1e4,1e5,1e6))

plot(sf_bins)
```

```{r}
ames_train %>% 
  mutate(BedroomAbvGr = as.factor(BedroomAbvGr)) %>% 
  group_by(BedroomAbvGr) %>% 
  ggplot(aes(x = GrLivArea, y = SalePrice)) + geom_point() + 
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  facet_wrap(BedroomAbvGr~.)
  
```
```{r}
feature = 'Alley'
ames_train %>% 
  mutate(feature = as.factor(feature)) %>% 
  group_by(feature) %>% 
  ggplot(aes(x = TotalSF, y = SalePrice)) + geom_point() + 
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  #scale_y_continuous(trans='log') +
  facet_wrap(feature)

feature = 'Alley'
ames_train %>% 
  #mutate(feature = as.factor(feature)) %>% 
  group_by(Alley) %>% 
  summarise(SalePrice = mean(SalePrice)) %>% 
  ggplot(aes(x = Alley, y = SalePrice)) + geom_col()
  #geom_smooth(method = "lm", se = TRUE, color = "red") +
  #scale_y_continuous(trans='log') +
  #facet_wrap(feature)
```
```{r}
library(scales)
```



```{r}
plot_pred_by_nom = function(nom_feature, predictor, scale_log = FALSE) {
  
  # x = ames_train[,predictor][[1]]
  # #x2 = ames_train[,nom_feature][[1]]
  # model = lm(ames_train$SalePrice ~ x)
  # predicted = predict(model)
  
  g = ames_train %>% 
    mutate(nom_feature = as.factor(nom_feature)) %>% 
    group_by(nom_feature) %>% 
    ggplot(aes_string(x = predictor, y = 'SalePrice'), environment=environment()) + 
    geom_point(shape=1,alpha=0.4) + 
    geom_smooth(method = "lm", se = TRUE, color = "red", linetype='dashed') +
    # geom_segment(aes(xend = x, yend=predicted), alpha=0.2) +
    # geom_point(aes(color=abs(model$residuals))) +
    # scale_color_continuous(low = "black", high = "red") +
    # guides(color=FALSE) +
    # geom_point(aes(y=predicted),shape=1, alpha=0.2) +
    facet_wrap(nom_feature) +
    labs(title = paste0(predictor,' v Sale Price for given ',nom_feature)) +
    theme_bw()
  
  
  model_name = paste0('SalePrice~',predictor)
  if(scale_log){
    g = g + 
      scale_y_continuous(trans='log') + 
      labs(title=paste0(predictor,' v Log Sale Price for given ',nom_feature))
    
    model_name = paste0('Log_SalePrice~',predictor)
  }
  
  file_name = paste0("./Linear Model/Ordinals/",nom_feature,'/',model_name,'.pdf')
  dir.create(paste0("./Linear Model/Ordinals/",nom_feature))
  # Save Scatter Plot
  ggsave(file_name, g)
  #dev.off()
  print(g)
  
}
```

```{r}
plot_pred_by_nom('MSSubClass',"TotalSF", scale_log=F)
```

```{r}
feature = 'HasPool'
ames_train %>% 
  mutate(feature = as.factor(feature)) %>% 
  #group_by(feature) %>% 
  ggplot(aes(x = TotalSF, y = SalePrice)) + geom_point() + 
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  facet_grid(feature) +
  scale_y_continuous(trans='log')
```

```{r}
feature = 'HasGarage'
ames_train %>% 
  mutate(feature = as.factor(feature)) %>% 
  group_by(feature) %>% 
  ggplot(aes(x = TotalSF, y = SalePrice)) + geom_point() + 
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  facet_wrap(feature) +
  scale_y_continuous(trans='log')
```

```{r}

boxplot(log(ames_train$SalePrice) ~ ames_train$MoSold, outline = FALSE)
boxplot(ames_train$SalePrice ~ ames_train$YrSold, outline = FALSE)
boxplot(log(ames_train$SalePrice) ~ ames_train$YearBuilt, outline = FALSE)
```



```{r}
feature = 'BsmtExposure'
ames_train %>% 
  mutate(feature = as.factor(feature)) %>% 
  group_by(feature) %>% 
  ggplot(aes(x = TotalSF, y = SalePrice)) + geom_point() + 
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  scale_y_continuous(trans='log') +
  facet_wrap(feature)
```
Lets look at Total square footage across different defined sub classes
```{r}
feature = 'MSSubClass'
g = ames_train %>% 
  #filter(feature < 70.0) %>% 
  mutate(feature = as.factor(feature)) %>% 
  group_by(feature) %>% 
  ggplot(aes(x = TotalSF, y = SalePrice)) + geom_point() + 
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  scale_y_continuous(trans='log') +
  facet_wrap(feature)


ggsave("./Linear Model/MsSubClass_logplot.pdf", g)
```



```{r}
feature = 'HouseStyle'
ames_train %>% 
  filter(AfterWW2==0) %>% 
  mutate(feature = as.factor(feature)) %>% 
  group_by(feature) %>% 
  ggplot(aes(x = GrLivArea, y = SalePrice)) + geom_point() + 
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  #geom_spline(color = "blue") +
  #scale_y_continuous(trans='log') +
  facet_wrap(feature)
```


```{r}

ggplot(aes(x, abs(model$residuals)), 
     xlab = x_name, 
     ylab = 'Residual Values Squared', 
     main = 'Squared Residuals compared to MSE')
abline(h=sqrt(mean(model$residuals^2)), lty = 2, lwd=3, col = 'red')
ss = smooth.spline(x, model$residuals^2, cv = TRUE)
lines(ss, lwd=2, col='blue')
abline(h=mean(ss$y), lwd=2, col='black')
```
```{r}
model <- eval(bquote(lm(.(f), data = ames_train)))
```


```{r}
d <- iris %>% dplyr::select(-Species)

# Fit the model
fit <- lm(Sepal.Width ~ ., data = iris)

# Obtain predicted and residual values
d$predicted <- predict(fit)
d$residuals <- residuals(fit)

# Create plot
d %>% 
  gather(key = "iv", value = "x", -Sepal.Width, -predicted, -residuals) %>%
  ggplot(aes(x = x, y = Sepal.Width)) +
  geom_segment(aes(xend = x, yend = predicted), alpha = .2) +
  geom_point(aes(color = residuals)) +
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +
  guides(color = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  facet_grid(~ iv, scales = "free_x") +
  theme_bw()
```

```{r}
library(broom)

# Steps 1 and 2
d <- lm(mpg ~ hp, data = mtcars) %>% 
       augment()

head(d)

# Steps 3 and 4
ggplot(d, aes(x = hp, y = mpg)) +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  geom_segment(aes(xend = hp, yend = .fitted), alpha = .2) +  # Note `.fitted`
  geom_point(aes(alpha = abs(.resid))) +  # Note `.resid`
  guides(alpha = FALSE) +
  geom_point(aes(y = .fitted), shape = 1) +  # Note `.fitted`
  theme_bw()
```



```{r}

# The new line of code


model = lm(log(SalePrice) ~ log(TotalSF), data=ames_train)
predicted = predict(model)

g = ames_train %>% 
  mutate(SalePrice = log(SalePrice), TotalSF = log(TotalSF)) %>% 
  ggplot(aes(x=TotalSF,y=SalePrice)) +
  #geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  geom_segment(aes(xend = TotalSF, yend=predicted), alpha=0.2) +
  geom_point(aes(color=abs(model$residuals))) +
  scale_color_continuous(low = "black", high = "red") +
  guides(color=FALSE) +
  geom_point(aes(y=predicted),shape=1) +
  theme_bw()

# Construction of Confidence Intervals

clower = predict(model, interval='confidence', level=0.95)[,2]
cupper = predict(model, interval='confidence', level=0.95)[,3]


plower = predict(model, interval='prediction', level=0.95)[,2]
pupper = predict(model, interval='prediction', level=0.95)[,3]

g + geom_ribbon(aes(ymin = clower, ymax = cupper), alpha=0.3, col = 'red') +
  geom_ribbon(aes(ymin=plower, ymax = pupper), alpha=0.2, col='blue')
```
Definitely a logarithimic relationship...


```{r}
qqnorm(model$residuals)
qqline(model$residuals)
```


```{r}
ames_train %>% 
  ggplot(aes(x = TotalSF, y = SalePrice)) +
  #geom_point() +
  geom_hex(bins=55)

```


## EDA

### Summary of Numerical Variables
```{r}
num_summary = num_train %>% describe()
write_csv(num_summary, './data/numerical_summary.csv')
num_summary
```

```{r}

skewed_features = colnames(train)[abs(num_summary$skew) > 3]
skewed_features
```
```{r}
train[,]
```



```{r}
model.saturated = lm(SalePrice ~ ., data = num_train)
```

```{r}
summary(model.saturated)
```

### Automating Variable Selection Process

According to Akaike Criterion
```{r}
library(MASS)
```

```{r}
model.empty = lm(SalePrice ~ 1, data = train) #The model with an intercept ONLY.
model.full = lm(SalePrice ~ ., data = train) #The model with ALL variables.
scope = list(lower = formula(model.empty), upper = formula(model.full))
```

```{r}
forwardAIC = step(model.empty, scope, direction = "forward", k = 2)
backwardAIC = step(model.full, scope, direction = "backward", k = 2)
bothAIC.empty = step(model.empty, scope, direction = "both", k = 2)
bothAIC.full = step(model.full, scope, direction = "both", k = 2)
```

```{r}
summary(bothAIC.empty)
```

```{r}
plot(bothAIC.full)
```


```{r}
influencePlot(bothAIC.full)
```
### Prediction Function
```{r}
make_pred = function(model, data) {
  data = data[,attr(model$terms, 'term.labels')]
  pred.band = predict(model, data, interval='prediction')
}
```



```{r}
sapply(num_train[rownames(inf),attr(bothAIC.full$terms, 'term.labels')],exp) %>% data.frame()
```

```{r}
summary(bothAIC.empty)
```



### Discrete Numerical Features EDA

```{r}
features = c('YearBuilt','YearRemodAdd','BsmtFullBath','BsmtHalfBath','Full','SalePrice')

train_dis = train[,features]
#test_dis = test[,features]

```

```{r}
clean_data_str = function(date_str) {
  mdy = mdy(date_str)
  ymd = ymd(date_str)
  
  if(is.na(mdy)){
    if(is.na(ymd)){
      return(NA)
    }
    else {return(ymd)}
  }
  else {return(mdy)}
}

```

```{r}
train_dis %>% ggplot(aes(x = YearBuilt, y = SalePrice)) + geom_point()
```

Split into test and train
```{r}
library(tidyverse)

x = model.matrix(SalePrice ~ ., ames)[, -1]
y = ames$SalePrice


set.seed(0)
train = sample(1:nrow(x), 7*nrow(x)/10)
y.test = y[-train]


```



```{r}
library(caret)
set.seed(0)
grid = 10^seq(5, -2, length = 100)
```

```{r}

cv.lasso.out = cv.glmnet(x[train,],y[train], lambda=grid, alpha=1, nfolds=10)
plot(cv.lasso.out)

```

```{r}
bestlambda.lasso = cv.lasso.out$lambda.min
bestlambda.lasso
log(bestlambda.lasso)
```

```{r}
lasso.bestlambdatrain = predict(lasso.models.train, s = bestlambda.lasso, newx = x[-train,])

mean((lasso.bestlambdatrain - y.test)^2)
```
```{r}
tmp_coeffs <- coef(cv.glmnet.fit, s = "lambda.min")
data.frame(name = tmp_coeffs@Dimnames[[1]][tmp_coeffs@i + 1], coefficient = tmp_coeffs@x)
```

```{r}
coefs.lasso = coef(cv.lasso.out)
```


```{r}
str(coefs.lasso)
```

```{r}
coefs.lasso@x
```

```{r}
gam1 = mgcv::gam(SalePrice ~ s(TotalSF, bs='ps', sp=0.6) + s(KitchenAbvGr, bs='ps', sp=0.6), data=ames_train)
```

```{r}
### GAM example using mgcv

library(mgcv)
library(ggplot2)
# fake data
n <- 50
sig <- 2
dat <- gamSim(1,n=n,scale=sig)

# P-spline smoothers (with lambda=0.6) used for x1 and x2; x3 is parametric.
b1 <- mgcv::gam(y ~ s(x1, bs='ps', sp=0.6) + s(x2, bs='ps', sp=0.6) + x3, data = dat)
summary(b1)
plot(b1)


# plot the smooth predictor function for x1 with ggplot to get a nicer looking graph
p <- predict(b1, type="lpmatrix")
beta <- coef(b1)[grepl("x1", names(coef(b1)))]
s <- p[,grepl("x1", colnames(p))] %*% beta
ggplot(data=cbind.data.frame(s, dat$x1), aes(x=dat$x1, y=s)) + geom_line()


# predict
newdf <- gamSim(1,n=n,scale=sig)
f <- predict(b1, newdata=newdf)


# select smoothing parameters with REML, using P-splines
b2 <- mgcv::gam(y ~ s(x1, bs='ps') + s(x2, bs='ps') + x3, data = dat, method="REML")

# select variables and smoothing parameters
b3 <- mgcv::gam(y ~ s(x0) + s(x1) + s(x2) + s(x3) , data = dat, method="REML", select=TRUE)

# loess smoothers with the gam package (restart R before loading gam)
library(gam)
b4 <- gam::gam(y ~ lo(x1, span=0.6) + lo(x2, span=0.6) + x3, data = dat)
summary(b4)
```

```{r}
feature = 'OverallQual'

df = ames_train %>% group_by(AfterWW2,HouseStyle) %>% summarise(price_med = median(SalePrice), n =n())

ames_train %>% 
  ggplot(aes(x = factor(HouseStyle), y = SalePrice)) + geom_boxplot() +
  geom_text(data=df, color='red',size=4,aes(y=price_med*0.8, label=n)) +
  facet_grid(.~factor(AfterWW2)) +
  labs() +
  #scale_y_continuous(trans='log') +
  theme_bw()

```

